{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Initial Processing\n",
    "Load the datasets, handle missing values, and perform initial data exploration including correlation analysis and outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Charger les données\n",
    "train_data = pd.read_csv('../ynov-data/train_housing_train.csv')\n",
    "valid_data = pd.read_csv('../ynov-data/train_housing_valid.csv')\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = train_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y = train_data['median_house_value']\n",
    "\n",
    "# Identifier les caractéristiques catégorielles et numériques\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Définir le préprocesseur\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder())\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Construire le pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Définir la grille de paramètres pour GridSearchCV\n",
    "param_grid = {\n",
    "    'preprocessor__num__poly__degree': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Utiliser GridSearchCV pour trouver les meilleurs paramètres\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions sur les données d'entraînement\n",
    "train_predictions = best_model.predict(X)\n",
    "\n",
    "# Évaluer le modèle sur les données d'entraînement\n",
    "train_rmse = np.sqrt(mean_squared_error(y, train_predictions))\n",
    "train_r2 = r2_score(y, train_predictions)\n",
    "print(f'Train RMSE: {train_rmse:.2f}')\n",
    "print(f'Train R²: {train_r2:.2f}')\n",
    "\n",
    "# Charger les données de validation\n",
    "X_valid = valid_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y_valid = valid_data['median_house_value']\n",
    "\n",
    "# Faire des prédictions sur les données de validation\n",
    "valid_predictions = best_model.predict(X_valid)\n",
    "\n",
    "# Évaluer le modèle sur les données de validation\n",
    "valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_predictions))\n",
    "valid_r2 = r2_score(y_valid, valid_predictions)\n",
    "print(f'Validation RMSE: {valid_rmse:.2f}')\n",
    "print(f'Validation R²: {valid_r2:.2f}')\n",
    "\n",
    "# Charger les données de test\n",
    "test = pd.read_csv('../ynov-data/test_housing.csv')\n",
    "X_test = test.drop('id', axis=1)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Créer le fichier de soumission\n",
    "submission = pd.read_csv('../ynov-data/submission.csv')\n",
    "submission['median_house_value'] = test_predictions\n",
    "submission.to_csv('../ynov-data/submission.csv', index=False)\n",
    "\n",
    "# Afficher les coefficients du modèle\n",
    "model = best_model.named_steps['regressor']\n",
    "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_})\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Selection\n",
    "Create new features through interactions, apply polynomial features selectively, and implement feature scaling and encoding. Focus on features with strong correlations to house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Charger les données\n",
    "train_data = pd.read_csv('../ynov-data/train_housing_train.csv')\n",
    "valid_data = pd.read_csv('../ynov-data/train_housing_valid.csv')\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = train_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y = train_data['median_house_value']\n",
    "\n",
    "# Identifier les caractéristiques catégorielles et numériques\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Définir le préprocesseur\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder())\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Créer le pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Définir la grille de paramètres pour GridSearchCV\n",
    "param_grid = {\n",
    "    'preprocessor__num__poly__degree': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Utiliser GridSearchCV pour trouver les meilleurs paramètres\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions sur les données d'entraînement\n",
    "train_predictions = best_model.predict(X)\n",
    "\n",
    "# Calculer les métriques de performance sur les données d'entraînement\n",
    "train_rmse = np.sqrt(mean_squared_error(y, train_predictions))\n",
    "train_r2 = r2_score(y, train_predictions)\n",
    "\n",
    "print(f'Train RMSE: {train_rmse:.2f}')\n",
    "print(f'Train R²: {train_r2:.2f}')\n",
    "\n",
    "# Faire des prédictions sur les données de validation\n",
    "X_valid = valid_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y_valid = valid_data['median_house_value']\n",
    "valid_predictions = best_model.predict(X_valid)\n",
    "\n",
    "# Calculer les métriques de performance sur les données de validation\n",
    "valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_predictions))\n",
    "valid_r2 = r2_score(y_valid, valid_predictions)\n",
    "\n",
    "print(f'Validation RMSE: {valid_rmse:.2f}')\n",
    "print(f'Validation R²: {valid_r2:.2f}')\n",
    "\n",
    "# Charger les données de test\n",
    "test = pd.read_csv('../ynov-data/test_housing.csv')\n",
    "X_test = test.drop('id', axis=1)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Créer le fichier de soumission\n",
    "submission = pd.read_csv('../ynov-data/submission.csv')\n",
    "submission['median_house_value'] = test_predictions\n",
    "submission.to_csv('../ynov-data/submission.csv', index=False)\n",
    "\n",
    "# Afficher les coefficients du modèle\n",
    "model = best_model.named_steps['regressor']\n",
    "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_})\n",
    "\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pipeline Creation\n",
    "Build a robust preprocessing pipeline with targeted imputation strategies, feature transformations, and regularization options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Charger les données\n",
    "train_data = pd.read_csv('../ynov-data/train_housing_train.csv')\n",
    "valid_data = pd.read_csv('../ynov-data/train_housing_valid.csv')\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = train_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y = train_data['median_house_value']\n",
    "\n",
    "# Identifier les caractéristiques catégorielles et numériques\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Définir le préprocesseur\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder())\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Construire le pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Définir la grille de paramètres pour GridSearchCV\n",
    "param_grid = {\n",
    "    'preprocessor__num__poly__degree': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Utiliser GridSearchCV pour trouver les meilleurs paramètres\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions sur les données d'entraînement\n",
    "train_predictions = best_model.predict(X)\n",
    "\n",
    "# Évaluer le modèle sur les données d'entraînement\n",
    "train_rmse = mean_squared_error(y, train_predictions, squared=False)\n",
    "train_r2 = r2_score(y, train_predictions)\n",
    "print(f'Train RMSE: {train_rmse:.2f}')\n",
    "print(f'Train R²: {train_r2:.2f}')\n",
    "\n",
    "# Préparer les données de validation\n",
    "X_valid = valid_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y_valid = valid_data['median_house_value']\n",
    "\n",
    "# Faire des prédictions sur les données de validation\n",
    "valid_predictions = best_model.predict(X_valid)\n",
    "\n",
    "# Évaluer le modèle sur les données de validation\n",
    "valid_rmse = mean_squared_error(y_valid, valid_predictions, squared=False)\n",
    "valid_r2 = r2_score(y_valid, valid_predictions)\n",
    "print(f'Validation RMSE: {valid_rmse:.2f}')\n",
    "print(f'Validation R²: {valid_r2:.2f}')\n",
    "\n",
    "# Charger les données de test\n",
    "test = pd.read_csv('../ynov-data/test_housing.csv')\n",
    "X_test = test.drop('id', axis=1)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Créer le fichier de soumission\n",
    "submission = pd.read_csv('../ynov-data/submission.csv')\n",
    "submission['median_house_value'] = test_predictions\n",
    "submission.to_csv('../ynov-data/submission.csv', index=False)\n",
    "\n",
    "# Afficher les coefficients du modèle\n",
    "model = best_model.named_steps['regressor']\n",
    "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_})\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Tuning\n",
    "Implement cross-validation, grid search for hyperparameter optimization, and evaluate different polynomial degrees and regularization strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Charger les données\n",
    "train_data = pd.read_csv('../ynov-data/train_housing_train.csv')\n",
    "valid_data = pd.read_csv('../ynov-data/train_housing_valid.csv')\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = train_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y = train_data['median_house_value']\n",
    "\n",
    "# Identifier les caractéristiques numériques et catégorielles\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Définir le préprocesseur\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder())\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Construire le pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Définir la grille de paramètres pour GridSearchCV\n",
    "param_grid = {\n",
    "    'preprocessor__num__poly__degree': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Utiliser GridSearchCV pour trouver les meilleurs paramètres\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions sur les données d'entraînement\n",
    "train_predictions = best_model.predict(X)\n",
    "\n",
    "# Évaluer le modèle sur les données d'entraînement\n",
    "train_rmse = np.sqrt(mean_squared_error(y, train_predictions))\n",
    "train_r2 = r2_score(y, train_predictions)\n",
    "print(f'Train RMSE: {train_rmse:.2f}')\n",
    "print(f'Train R²: {train_r2:.2f}')\n",
    "\n",
    "# Préparer les données de validation\n",
    "X_valid = valid_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y_valid = valid_data['median_house_value']\n",
    "\n",
    "# Faire des prédictions sur les données de validation\n",
    "valid_predictions = best_model.predict(X_valid)\n",
    "\n",
    "# Évaluer le modèle sur les données de validation\n",
    "valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_predictions))\n",
    "valid_r2 = r2_score(y_valid, valid_predictions)\n",
    "print(f'Validation RMSE: {valid_rmse:.2f}')\n",
    "print(f'Validation R²: {valid_r2:.2f}')\n",
    "\n",
    "# Charger les données de test\n",
    "test = pd.read_csv('../ynov-data/test_housing.csv')\n",
    "X_test = test.drop('id', axis=1)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Préparer le fichier de soumission\n",
    "submission = pd.read_csv('../ynov-data/submission.csv')\n",
    "submission['median_house_value'] = test_predictions\n",
    "submission.to_csv('../ynov-data/submission.csv', index=False)\n",
    "\n",
    "# Afficher les coefficients du modèle\n",
    "model = best_model.named_steps['regressor']\n",
    "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_})\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Predictions\n",
    "Calculate performance metrics, analyze residuals, identify influential features, and generate final predictions for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Charger les données\n",
    "train_data = pd.read_csv('../ynov-data/train_housing_train.csv')\n",
    "valid_data = pd.read_csv('../ynov-data/train_housing_valid.csv')\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = train_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y = train_data['median_house_value']\n",
    "\n",
    "# Identifier les caractéristiques catégorielles et numériques\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Définir le préprocesseur\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder())\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Construire le pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Définir la grille de paramètres pour GridSearchCV\n",
    "param_grid = {\n",
    "    'preprocessor__num__poly__degree': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Utiliser GridSearchCV pour trouver les meilleurs paramètres\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions sur les données d'entraînement\n",
    "train_predictions = best_model.predict(X)\n",
    "\n",
    "# Évaluer le modèle sur les données d'entraînement\n",
    "train_rmse = mean_squared_error(y, train_predictions, squared=False)\n",
    "train_r2 = r2_score(y, train_predictions)\n",
    "\n",
    "print(f'Train RMSE: {train_rmse:.2f}')\n",
    "print(f'Train R²: {train_r2:.2f}')\n",
    "\n",
    "# Faire des prédictions sur les données de validation\n",
    "X_valid = valid_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y_valid = valid_data['median_house_value']\n",
    "valid_predictions = best_model.predict(X_valid)\n",
    "\n",
    "# Évaluer le modèle sur les données de validation\n",
    "valid_rmse = mean_squared_error(y_valid, valid_predictions, squared=False)\n",
    "valid_r2 = r2_score(y_valid, valid_predictions)\n",
    "\n",
    "print(f'Validation RMSE: {valid_rmse:.2f}')\n",
    "print(f'Validation R²: {valid_r2:.2f}')\n",
    "\n",
    "# Charger les données de test\n",
    "test = pd.read_csv('../ynov-data/test_housing.csv')\n",
    "X_test = test.drop('id', axis=1)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Créer le fichier de soumission\n",
    "submission = pd.read_csv('../ynov-data/submission.csv')\n",
    "submission['median_house_value'] = test_predictions\n",
    "submission.to_csv('../ynov-data/submission.csv', index=False)\n",
    "\n",
    "# Afficher les coefficients du modèle\n",
    "model = best_model.named_steps['regressor']\n",
    "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_})\n",
    "\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coefficients)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
